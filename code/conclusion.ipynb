{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef68eb2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003885,
     "end_time": "2022-10-10T14:09:36.078448",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.074563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "* **Name** Acharya Atul\n",
    "* **Matriculation Number** U1923502C\n",
    "* **Email** atul001@e.ntu.edu.sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fa9c7",
   "metadata": {
    "papermill": {
     "duration": 0.002889,
     "end_time": "2022-10-10T14:09:36.084538",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.081649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part A - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0d4c9",
   "metadata": {
    "papermill": {
     "duration": 0.002452,
     "end_time": "2022-10-10T14:09:36.089845",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.087393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### We now have a classifier that predicts the speech polarity. What are some limitations of the current approach (using FFNs to model such engineered features)?\n",
    "> Many deep learning neural networks contain hard-coded data processing, feature extraction, and feature engineering. However, when we perform feature engineering we might loose some important information that the neural network could have used to model the data better. The raw audio signal could've been fed to a neural network architecture such as an RNN since it can recognize the data's sequential characteristics and use patterns to give us a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef21f6b",
   "metadata": {
    "papermill": {
     "duration": 0.002456,
     "end_time": "2022-10-10T14:09:36.094960",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.092504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Out of the parameters that were tuned, which was most impactful in terms of improving the model performance and what could be some reasons for that? \n",
    "\n",
    ">The differences in the accuracies produced with different hidden layer sizes was much more than what the batch sizes produced. Tuning the hidden layer size increases the accuracy from 0.62 to 0.67 whereas tuning the batch sizes produced accuracies in the range 0.64 - 0.66. <br>\n",
    "This could be because adding more neurons in the hidden layer allowed the model to learn better features and identify more relationships between the variables which could not have been done with fewer neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8dffc",
   "metadata": {
    "papermill": {
     "duration": 0.002498,
     "end_time": "2022-10-10T14:09:36.100091",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.097593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Considering that audio tracks are originally waveforms, what are some alternative approaches to achieve the goal of genre classification? What kind of neural network architectures will be used instead?\n",
    "\n",
    "> We could use recurrent neural network architectures to perform the task of genre classification. Waveform data is sequential in nature and the RNN architectures such as LSTM or GRUs would learn to identify the data's sequential characteristics and identify new relationsips.\n",
    "\n",
    ">If we have the images of the spectrograms, we could use a Convolutional Neural Network for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ae553",
   "metadata": {
    "papermill": {
     "duration": 0.002432,
     "end_time": "2022-10-10T14:09:36.105152",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.102720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### What other datasets and tasks can this approach of modeling waveform data be used for? What changes to the pipeline, if any, will you have to make when approaching these problems?\n",
    "\n",
    ">Using this approach of modeling waveforms, we can do many other tasks such as Gender Recognition from voice, Emotion detection (here the problem is multi-class classification), Music Genre Classification, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823d765",
   "metadata": {
    "papermill": {
     "duration": 0.002402,
     "end_time": "2022-10-10T14:09:36.110530",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.108128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Part B - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b299c",
   "metadata": {
    "papermill": {
     "duration": 0.002388,
     "end_time": "2022-10-10T14:09:36.115591",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.113203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### In Q1, we compared a linear regression model to an equivalent neural network architecture and also saw how adding a hidden layer changes model performance. In Q2, we saw how adding an Embedding layer introduces more learnable parameters to the neural network. What other benefits do neural networks have over other machine learning approaches? In cases where neural networks perform better, is it possible to modify ‘traditional’ machine learning algorithms to close up the gap? \n",
    "\n",
    ">Neural Networks are very powerful and are used in a wide range of applications. Neural Networks most of the time win over traditional machine learning algorithms. The ability of neural networks to learn rich features on its own without relying too much on human engineered features makes it superior than traditional machine learning algorithms. For complex tasts, machine learning algorithms require a lot of preprocessing and feature generation in order to perform decently well. \n",
    "\n",
    ">***Example***: Consider the task of Image Classification. Before CNNs were introduced, standard algorithms like K-neirest Neighbours Classifiers were used for image recognition. The had no spatial informations about the pixels and performed much worse than humans. Even popular algorithms such as Gradient Boosting Trees did not perform as well as CNNs and required a lot of feature engineering. CNNs take the raw image and transform these images using convolutions and learn much better features on its own and they have a way of encoding spatial information of the pixels.\n",
    "\n",
    ">Neural Networks can also perform multiple tasks in parallel without affecting the system performance. As we saw in the lecture, a feed forward neural network with an output layer of 2 with Linear activation function can perform 2 tasks. One neuron can predict the age of a person and the other output neuron could predict the weight of a person. Thus neural networks can learn multiple different targets at the same time. This is called Multi-Task Learning.\n",
    "***Example***: Object Detection. The neural network must identify the image in the picture as well as generate its bounding boxes.\n",
    "\n",
    "> These are only some of the benefits neural networks have over other machine learning algorithms and there are definitely much more advantages.\n",
    "\n",
    ">In the case where neural networks perform better, we can try to modify tradional machine learning algorithms to close the gap. We could try using the features generated by the neural network as inputs to these machine learning algorithms. Even though these features may only be meaningful to the neural network, they still contain useful information which the machine learning algorithms can leverage on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4fed2",
   "metadata": {
    "papermill": {
     "duration": 0.00286,
     "end_time": "2022-10-10T14:09:36.121079",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.118219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### In Q2, we tried out another approach of model tuning. KerasTuner offers many other algorithms – how do Bayesian optimisation or HyperBand work? Are they necessarily better than random search? Also, is random search better than grid search?\n",
    ">Unlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations. The number of trials in this approach is determined by the user. Bayesian search has been shown to outperform GridSearch and RandoSearch. This method creates a probabilistic model, in which it maps hyperparameters to their corresponding score probability. Instead of painstakingly trying every hyperparameter set or testing hyperparameter sets at random, the Bayesian optimization method can converge to the optimal hyperparameters. Thus, the best hyperparameters can be obtained without exploring the entire sample space. With the Bayesian optimization method, users do not have to endure long run times that come from evaluating every hyperparameter set. They also do not have to incorporate randomness and risk missing the optimal solution.\n",
    "\n",
    ">However Bayesian optimization does have its own drawback. Since this is an informed learning method, additional time is required to determine the next hyperparameters to evaluate based on the results of the previous iterations. At the expense of minimizing the number of trials, Bayesian optimization requires more time for each iteration. Hence, it is not necessary better than random search.\n",
    "\n",
    ">Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. It is similar to grid search, and yet it has proven to yield better results comparatively. Random search is able to find a diverse combination of hyperparameters than Grid search since it is not restricted by the user defined values for hyperparameters. As random values are selected at each instance, it is highly likely that the whole of action space has been reached because of the randomness, which takes a huge amount of time to cover every aspect of the combination during grid search. Thus random search is most of the time better than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b73f86",
   "metadata": {
    "papermill": {
     "duration": 0.002418,
     "end_time": "2022-10-10T14:09:36.126242",
     "exception": false,
     "start_time": "2022-10-10T14:09:36.123824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.49587,
   "end_time": "2022-10-10T14:09:36.854198",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-10T14:09:26.358328",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
